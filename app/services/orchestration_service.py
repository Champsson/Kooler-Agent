from . import assistant_service, tts_service, cache_service
from ..utils import get_logger
import time

logger = get_logger(__name__)

# In-memory store for conversation threads (replace with persistent storage like Redis in production)
# Key: Twilio CallSid or From number (for SMS), Value: OpenAI Thread ID
conversation_threads = {}

def get_thread_id(session_key: str) -> str:
    """Gets or creates an OpenAI thread ID for a given session key (CallSid or From number)."""
    thread_id = conversation_threads.get(session_key)
    if not thread_id:
        try:
            thread = assistant_service.create_thread()
            thread_id = thread.id
            conversation_threads[session_key] = thread_id
            logger.info(f"Created new thread {thread_id} for session {session_key}")
        except Exception as e:
            logger.error(f"Failed to create OpenAI thread for session {session_key}: {e}")
            raise ConnectionError(f"Failed to initialize conversation thread: {e}")
    else:
        logger.info(f"Using existing thread {thread_id} for session {session_key}")
    return thread_id

def handle_interaction(session_key: str, user_input: str) -> str:
    """Handles a user interaction (voice or SMS), orchestrates Assistant call, and returns the response text.

    Args:
        session_key (str): Unique identifier for the session (e.g., Twilio CallSid for voice, From number for SMS).
        user_input (str): The text input from the user.

    Returns:
        str: The text response generated by the assistant.
    """
    start_time = time.time()
    logger.info(f"Handling interaction for session {session_key}. Input: 	{user_input[:50]}...	")

    # --- Optional: Check Cache --- 
    cache_start = time.time()
    cache_key = f"response::{session_key}::{user_input[:100]}" # Create a cache key
    cached_response = cache_service.get_cached_response(cache_key)
    cache_end = time.time()
    logger.debug(f"Cache check took: {cache_end - cache_start:.4f}s")
    if cached_response:
        logger.info("Returning cached response.")
        total_time = time.time() - start_time
        logger.info(f"Total interaction time (cached): {total_time:.4f}s")
        return cached_response

    try:
        # 1. Get or Create Thread ID
        thread_start = time.time()
        thread_id = get_thread_id(session_key)
        thread_end = time.time()
        logger.debug(f"Get/Create thread took: {thread_end - thread_start:.4f}s")

        # 2. Add user message to thread
        add_msg_start = time.time()
        assistant_service.add_message_to_thread(thread_id, user_input)
        add_msg_end = time.time()
        logger.debug(f"Add message took: {add_msg_end - add_msg_start:.4f}s")

        # 3. Get or Create Assistant
        get_asst_start = time.time()
        assistant = assistant_service.get_or_create_assistant()
        get_asst_end = time.time()
        logger.debug(f"Get/Create assistant took: {get_asst_end - get_asst_start:.4f}s (may use cache)")
        if not assistant:
             raise ConnectionError("Failed to get or create OpenAI Assistant.")

        # 4. Run the assistant using streaming
        run_start = time.time()
        assistant_response_text, run_status = assistant_service.run_assistant_stream(thread_id, assistant.id)
        run_end = time.time()
        logger.debug(f"Run assistant stream took: {run_end - run_start:.4f}s")

        # 5. Check the final status and response
        if run_status == "completed":
            if not assistant_response_text:
                 logger.warning(f"Assistant run completed but no response text accumulated for thread {thread_id}.")
                 assistant_response_text = "I seem to have lost my train of thought. Could you please ask again?" # Fallback response
        else:
            # The run_assistant_stream function already includes error logging
            # Use the error message returned by the function
            pass # assistant_response_text already contains the error message

        # --- Optional: Store Response in Cache --- 
        set_cache_start = time.time()
        cache_service.set_cached_response(cache_key, assistant_response_text)
        set_cache_end = time.time()
        logger.debug(f"Set cache took: {set_cache_end - set_cache_start:.4f}s")

        total_time = time.time() - start_time
        logger.info(f"Generated response for session {session_key}: 	{assistant_response_text[:50]}...	")
        logger.info(f"Total interaction time (uncached): {total_time:.4f}s")
        return assistant_response_text

    except ConnectionError as ce:
        logger.error(f"Connection error during interaction handling for {session_key}: {ce}", exc_info=True)
        return f"Sorry, I encountered a connection issue. Please try again later. ({ce})"
    except Exception as e:
        logger.error(f"Unexpected error during interaction handling for {session_key}: {e}", exc_info=True)
        return "I encountered an unexpected internal error. Please try again later."

# Example usage (for testing)
if __name__ == "__main__":
    test_session = "test_session_123"
    test_input_1 = "How do I reset my garage door opener?"
    test_input_2 = "What are your business hours?"

    print(f"--- Test 1: {test_input_1} ---")
    response1 = handle_interaction(test_session, test_input_1)
    print(f"Response 1: {response1}")

    # Test caching
    print(f"\n--- Test 1 (Cached): {test_input_1} ---")
    start_time = time.time()
    response1_cached = handle_interaction(test_session, test_input_1)
    end_time = time.time()
    print(f"Response 1 (Cached): {response1_cached}")
    print(f"Time taken: {end_time - start_time:.4f} seconds")

    print(f"\n--- Test 2: {test_input_2} ---")
    response2 = handle_interaction(test_session, test_input_2)
    print(f"Response 2: {response2}")

